{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from math import sqrt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf \n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic\n",
    "\n",
    "def basic_avg(df):\n",
    "    \n",
    "    df['tstp'] = pd.to_datetime(df['tstp'],format = '%d/%m/%Y %H:%M')\n",
    "    \n",
    "    #Separate into train and test\n",
    "    print('separating into train and test')\n",
    "    train = df[:33600]\n",
    "    test = df[33600:]\n",
    "    y_test = test['energy(kWh/hh)'].to_list()\n",
    "               \n",
    "    # groupby (in our case this is the prediction)\n",
    "    print('Predicting')\n",
    "    prediction_hh = []\n",
    "    mean = train[\"energy(kWh/hh)\"].mean()\n",
    "    prediction_hh.append(mean)\n",
    "    #Since our test is 4 weeks long, our prediction has to be 4 weeks long\n",
    "    y_pred = prediction_hh*1488   \n",
    "        \n",
    "    #Metrics\n",
    "    print('Calculating metric')\n",
    "    rmsle_i = rmsle(y_test,y_pred)\n",
    "    rmse_i = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weekly\n",
    "\n",
    "def average_week(df):\n",
    "    \n",
    "    df['tstp'] = pd.to_datetime(df['tstp'],format = '%d/%m/%Y %H:%M')\n",
    "    df['year'] = pd.DatetimeIndex(df['tstp']).year\n",
    "    \n",
    "    #Separate into train and test\n",
    "    print('Separating into train and test')\n",
    "    train = df[:33648]\n",
    "    test = df[33648:34992]\n",
    "    y_test = test['energy(kWh/hh)'].to_list()\n",
    "        \n",
    "    #create columns for the grouping by tstp\n",
    "    train['day_of_week'] = pd.DatetimeIndex(train['tstp']).dayofweek\n",
    "    train['hour'] = pd.DatetimeIndex(train['tstp']).hour\n",
    "    train['minute'] = pd.DatetimeIndex(train['tstp']).minute\n",
    "        \n",
    "    #Predict\n",
    "    print('Predicting')\n",
    "    # groupby (in our case this is the prediction)\n",
    "    prediction_hh = train.groupby(['day_of_week','hour','minute']).mean().reset_index()\n",
    "    del prediction_hh['year']\n",
    "    #Since our test is 4 weeks long, our prediction has to be 4 weeks long\n",
    "    y_pred_sem1 = prediction_hh['energy(kWh/hh)']\n",
    "    y_pred_sem2 = prediction_hh['energy(kWh/hh)']\n",
    "    y_pred_sem3 = prediction_hh['energy(kWh/hh)']\n",
    "    y_pred_sem4 = prediction_hh['energy(kWh/hh)']\n",
    "    frames = [y_pred_sem1,y_pred_sem2,y_pred_sem3,y_pred_sem4]\n",
    "    y_pred = pd.concat(frames).to_list()\n",
    "            \n",
    "    #Metrics\n",
    "    print('Calculating metrics')\n",
    "    rmsle_i = rmsle(y_test,y_pred)\n",
    "    rmse_i = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA\n",
    "\n",
    "def arima_loop(dataset):\n",
    "\n",
    "    # split into train and test sets\n",
    "    print('Separating into train and test')\n",
    "    train = dataset.iloc[:33600]\n",
    "    test = dataset.iloc[33600:]\n",
    "    y_test = test['energy(kWh/hh)'].to_list()\n",
    "    \n",
    "    # Create Model\n",
    "    print('Creating model')\n",
    "    arima_model = ARIMA(train,order = (2,0,0)).fit()\n",
    "\n",
    "    #Predict and save results\n",
    "    print('Prediciting and saving results')\n",
    "    ARIMA_prediction = arima_model.predict(start=33600, end =35087,typ= 'levels')\n",
    "    y_pred = ARIMA_prediction.to_list()\n",
    "    \n",
    "    #Metrics\n",
    "    print('Calculating metrics')\n",
    "    rmsle_i = rmsle(y_test,y_pred)\n",
    "    rmse_i = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Univariate\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def lstm_univariate_loop(dataset):\n",
    "    \n",
    "    random.seed(1)\n",
    "    dataset = df.values\n",
    "    dataset = df.astype('float32')\n",
    "    \n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "    \n",
    "    # split into train and test sets\n",
    "    print('Spliting into train and test')\n",
    "    train_size = int(33600)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "    \n",
    "    # reshape into X=t and Y=t+1\n",
    "    look_back = 3\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    testY_copy = testY.copy()\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "    \n",
    "    # create and fit the LSTM network\n",
    "    print('Creating and fitting the LSTM network')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=rmsle_loss, optimizer='adam', metrics = [rmsle_loss])\n",
    "    # simple early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "    history = model.fit(trainX, trainY, epochs=15, batch_size=70,validation_split=0.04, verbose=1, shuffle=False, callbacks = [es])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # make predictions\n",
    "    print('Making predictions')\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    \n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform([trainY])\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform([testY])\n",
    "    \n",
    "    #Metrics\n",
    "    print('Calculating metrics')\n",
    "    rmsle_i = rmsle(testY_copy,testPredict)\n",
    "    rmse_i = sqrt(mean_squared_error(testY_copy, testPredict))\n",
    "    \n",
    "    return testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Household\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
