{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from math import sqrt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf \n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic\n",
    "\n",
    "def basic_avg(df):\n",
    "    \n",
    "    df['tstp'] = pd.to_datetime(df['tstp'],format = '%d/%m/%Y %H:%M')\n",
    "    \n",
    "    #Separate into train and test\n",
    "    print('separating into train and test')\n",
    "    train = df[:33600]\n",
    "    test = df[33600:]\n",
    "    y_test = test['energy(kWh/hh)'].to_list()\n",
    "               \n",
    "    # groupby (in our case this is the prediction)\n",
    "    print('Predicting')\n",
    "    prediction_hh = []\n",
    "    mean = train[\"energy(kWh/hh)\"].mean()\n",
    "    prediction_hh.append(mean)\n",
    "    #Since our test is 4 weeks long, our prediction has to be 4 weeks long\n",
    "    y_pred = prediction_hh*1488   \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weekly\n",
    "\n",
    "def average_week(df):\n",
    "    \n",
    "    df['tstp'] = pd.to_datetime(df['tstp'],format = '%d/%m/%Y %H:%M')\n",
    "    df['year'] = pd.DatetimeIndex(df['tstp']).year\n",
    "    \n",
    "    #Separate into train and test\n",
    "    print('Separating into train and test')\n",
    "    train = df[:33648]\n",
    "    test = df[33648:34992]\n",
    "    y_test = test['energy(kWh/hh)'].to_list()\n",
    "        \n",
    "    #create columns for the grouping by tstp\n",
    "    train['day_of_week'] = pd.DatetimeIndex(train['tstp']).dayofweek\n",
    "    train['hour'] = pd.DatetimeIndex(train['tstp']).hour\n",
    "    train['minute'] = pd.DatetimeIndex(train['tstp']).minute\n",
    "        \n",
    "    #Predict\n",
    "    print('Predicting')\n",
    "    # groupby (in our case this is the prediction)\n",
    "    prediction_hh = train.groupby(['day_of_week','hour','minute']).mean().reset_index()\n",
    "    del prediction_hh['year']\n",
    "    #Since our test is 4 weeks long, our prediction has to be 4 weeks long\n",
    "    y_pred_sem1 = prediction_hh['energy(kWh/hh)']\n",
    "    y_pred_sem2 = prediction_hh['energy(kWh/hh)']\n",
    "    y_pred_sem3 = prediction_hh['energy(kWh/hh)']\n",
    "    y_pred_sem4 = prediction_hh['energy(kWh/hh)']\n",
    "    frames = [y_pred_sem1,y_pred_sem2,y_pred_sem3,y_pred_sem4]\n",
    "    y_pred = pd.concat(frames).to_list()\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA\n",
    "\n",
    "def arima_loop(dataset):\n",
    "\n",
    "    # split into train and test sets\n",
    "    print('Separating into train and test')\n",
    "    train = dataset.iloc[:33600]\n",
    "    test = dataset.iloc[33600:]\n",
    "    y_test = test['energy(kWh/hh)'].to_list()\n",
    "    \n",
    "    # Create Model\n",
    "    print('Creating model')\n",
    "    arima_model = ARIMA(train,order = (2,0,0)).fit()\n",
    "\n",
    "    #Predict and save results\n",
    "    print('Prediciting and saving results')\n",
    "    ARIMA_prediction = arima_model.predict(start=33600, end =35087,typ= 'levels')\n",
    "    y_pred = ARIMA_prediction.to_list()\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Univariate\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def lstm_univariate_loop(dataset):\n",
    "    \n",
    "    random.seed(1)\n",
    "    dataset = df.values\n",
    "    dataset = df.astype('float32')\n",
    "    \n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "    \n",
    "    # split into train and test sets\n",
    "    print('Spliting into train and test')\n",
    "    train_size = int(33600)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "    \n",
    "    # reshape into X=t and Y=t+1\n",
    "    look_back = 3\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    testY_copy = testY.copy()\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "    \n",
    "    # create and fit the LSTM network\n",
    "    print('Creating and fitting the LSTM network')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=rmsle_loss, optimizer='adam', metrics = [rmsle_loss])\n",
    "    # simple early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "    history = model.fit(trainX, trainY, epochs=15, batch_size=70,validation_split=0.04, verbose=1, shuffle=False, callbacks = [es])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # make predictions\n",
    "    print('Making predictions')\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    \n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform([trainY])\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform([testY])\n",
    "\n",
    "    \n",
    "    return testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Multivariate\n",
    "\n",
    "def rmsle_loss(y_pred, y_true):\n",
    "    return K.sqrt(K.mean(K.square(tf.math.log1p(y_pred) - tf.math.log1p(y_true))))\n",
    "\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def lstm_multivariate(filename):\n",
    "    random.seed(1)\n",
    "    values = filename.values\n",
    "    \n",
    "    # integer encode direction\n",
    "    encoder = LabelEncoder()\n",
    "    values[:,8] = encoder.fit_transform(values[:,8])\n",
    "    # ensure all data is float\n",
    "    values = values.astype('float32')\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    # frame as supervised learning\n",
    "    reframed = series_to_supervised(scaled, 1, 1)\n",
    "    # drop columns we don't want to predict\n",
    "    reframed.drop(reframed.columns[[10,11,11,12,13,14,15,16,17]], axis=1, inplace=True)\n",
    "    print(reframed.head())\n",
    "    \n",
    "    \n",
    "    # split into train and test sets\n",
    "    print(\"Splitting into train and test\")\n",
    "    values = reframed.values\n",
    "    n_train_hours = 33600\n",
    "    train = values[:n_train_hours, :]\n",
    "    test = values[n_train_hours:, :]\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, :-1], train[:, -1]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    \n",
    "    \n",
    "    # design network\n",
    "    print(\"Designing network\")\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=rmsle_loss, optimizer='adam', metrics = [rmsle_loss])\n",
    "    # simple early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "   \n",
    "    # fit network\n",
    "    print(\"Fitting network\")\n",
    "    history = model.fit(train_X, train_y, epochs=15, batch_size=70,validation_split=0.04, verbose=1, shuffle=False, callbacks = [es])\n",
    "    \n",
    "\n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "    \n",
    "    \n",
    "    return rmsle_i,rmse_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Household\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
