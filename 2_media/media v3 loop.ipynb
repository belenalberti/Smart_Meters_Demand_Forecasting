{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from sklearn import metrics\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def media(filename):\n",
    "    df['tstp'] = pd.to_datetime(df['tstp'],format = '%d/%m/%Y %H:%M')\n",
    "    df['year'] = pd.DatetimeIndex(df['tstp']).year\n",
    "    groupby = df.groupby(['tstp','year']).mean().reset_index()\n",
    "    \n",
    "    if len(groupby) == 35088:\n",
    "        #Separate into train and test\n",
    "        train = group_by_tstp[:33648]\n",
    "        test = group_by_tstp[33648:34992]\n",
    "        y_test = test['energy'].to_list()\n",
    "        \n",
    "        #create columns for the grouping by tstp\n",
    "        train['day_of_week'] = pd.DatetimeIndex(train['tstp']).dayofweek\n",
    "        train['hour'] = pd.DatetimeIndex(train['tstp']).hour\n",
    "        train['minute'] = pd.DatetimeIndex(train['tstp']).minute\n",
    "        \n",
    "        # groupby (in our case this is the prediction)\n",
    "        prediction_hh = train.groupby(['day_of_week','hour','minute']).mean().reset_index()\n",
    "        del prediction_hh['year']\n",
    "        \n",
    "        #Since our test is 4 weeks long, our prediction has to be 4 weeks long as well\n",
    "        y_pred_sem1 = prediction_hh['energy']\n",
    "        y_pred_sem2 = prediction_hh['energy']\n",
    "        y_pred_sem3 = prediction_hh['energy']\n",
    "        y_pred_sem4 = prediction_hh['energy']\n",
    "        frames = [y_pred_sem1,y_pred_sem2,y_pred_sem3,y_pred_sem4]\n",
    "        y_pred = pd.concat(frames).to_list()\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suma_gen(filename):    \n",
    "    del filename['LCLid']\n",
    "    filename['tstp'] = pd.to_datetime(filename['tstp'],format = '%Y-%m-%d %H:%M')\n",
    "    filename = filename[filename['energy(kWh/hh)'] != \"Null\"]\n",
    "    filename['energy(kWh/hh)'] = filename['energy(kWh/hh)'].astype(float)\n",
    "    filename['year'] = pd.DatetimeIndex(filename['tstp']).year\n",
    "    filename = filename[(filename['year'] == 2012) | (filename['year'] == 2013)]   \n",
    "    group_by_tstp = filename.groupby(['tstp','year']).sum().reset_index()\n",
    "\n",
    "    if len(group_by_tstp) == 35088:\n",
    "        #Separate into train and test\n",
    "        train = group_by_tstp[:33648]\n",
    "        test = group_by_tstp[33648:34992]\n",
    "\n",
    "        #create columns for the grouping by tstp\n",
    "        train['day_of_week'] = pd.DatetimeIndex(train['tstp']).dayofweek\n",
    "        train['hour'] = pd.DatetimeIndex(train['tstp']).hour\n",
    "        train['minute'] = pd.DatetimeIndex(train['tstp']).minute\n",
    "        \n",
    "        # groupby (in our case this is the prediction)\n",
    "        prediction_gen = train.groupby(['day_of_week','hour','minute']).sum().reset_index()\n",
    "        del prediction_gen['year']\n",
    "        \n",
    "        y_test_sem1 = prediction_gen['energy']\n",
    "        y_test_sem2 = prediction_gen['energy']\n",
    "        y_test_sem3 = prediction_gen['energy']\n",
    "        y_test_sem4 = prediction_gen['energy']\n",
    "        frames = [y_test_sem1,y_test_sem2,y_test_sem3,y_test_sem4]\n",
    "        y_test = pd.concat(frames).to_list()\n",
    "        \n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the blocks and concatenate them into one database\n",
    "path = '/Users/mariabelenalberti/OneDrive - Universidad Torcuato Di Tella/Tesis/smart-meters-in-london/halfhourly_dataset' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "lista = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    lista.append(df)\n",
    "halfhourly_consumption = pd.concat(lista, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test\n",
    "suma_gen(halfhourly_consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the blocks and concatenate them into one database\n",
    "path = '/Users/mariabelenalberti/OneDrive - Universidad Torcuato Di Tella/Tesis/code_datasets/1_data_frames/univariate/households_df' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "y_pred = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    y_pred = media(df)\n",
    "    y_pred.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.concat(y_pred, axis=1, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
